{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15113873",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai dotenv google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load your .env file\n",
    "load_dotenv()  # reads .env into environment variables\n",
    "\n",
    "# Get the API key from the environment\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "# Create the OpenAI client with the key\n",
    "# client = OpenAI(api_key=api_key)\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeeec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# # Configure Gemini client with your API key\n",
    "# genai.configure(api_key=\"your-gemini-api-key\")  # Replace with your real key\n",
    "\n",
    "# Gemini-compatible version of get_completion\n",
    "def get_completion(prompt, model=\"gemini-2.0-flash\"):\n",
    "    model = genai.GenerativeModel(model)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc67d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def messages_to_prompt(messages):\n",
    "    \"\"\"\n",
    "    Convert chat-style messages (system/user/assistant) into a single prompt string\n",
    "    for Gemini, which expects plain text prompts.\n",
    "    \"\"\"\n",
    "    prompt_parts = []\n",
    "    for msg in messages:\n",
    "        role = msg['role']\n",
    "        content = msg['content']\n",
    "        if role == 'system':\n",
    "            prompt_parts.append(content + \"\\n\")\n",
    "        elif role == 'user':\n",
    "            prompt_parts.append(f\"User: {content}\")\n",
    "        elif role == 'assistant':\n",
    "            prompt_parts.append(f\"Assistant: {content}\")\n",
    "    prompt_parts.append(\"Assistant:\")  # prompt Gemini to continue as Assistant\n",
    "    return \"\\n\".join(prompt_parts)\n",
    "\n",
    "def get_completion(prompt, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Simple completion from a user prompt string (no chat history)\n",
    "    \"\"\"\n",
    "    model_obj = genai.GenerativeModel(model)\n",
    "    response = model_obj.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beffea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "# {'role':'user', 'parts':['You are an assistant that speaks like Shakespeare.']},    \n",
    "# {'role':'user', 'parts':['tell me a joke']},   \n",
    "{'role':'assistant', 'parts':['my name is Shakespeare, and I am here to assist thee.']},   \n",
    "{'role':'user', 'parts':['what/s your name again?']}  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = get_completion(messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
